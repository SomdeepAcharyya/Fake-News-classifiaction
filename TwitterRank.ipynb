{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterRank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CRb5OlICmmNidMWUW9qXGJQkjqTM8T55",
      "authorship_tag": "ABX9TyM4iVt1vfrdKu7vpXJVe8Pa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Fake-News-classifiaction/blob/main/TwitterRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEmy_jtjWrz6",
        "outputId": "df13fcf3-c571-482c-8de3-b0034b7d9158"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('fivethirtyeight')\n",
        "import networkx as nx\n",
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "import csv\n",
        "from itertools import chain\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nlp = spacy.load('en')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "spc = spacy.load('en_core_web_sm')\n",
        "import http.client\n",
        "import urllib.parse\n",
        "from urllib.request import urlopen\n",
        "import math\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED)\n",
        "import datetime\n",
        "from time import strptime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYqYNKGMRBsC"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import scipy.stats\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import networkx as nx\n",
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "class TwitterRank:\n",
        "\n",
        "  def update_progress(progress):\n",
        "      bar_length = 80\n",
        "      if isinstance(progress, int):\n",
        "          progress = float(progress)\n",
        "      if not isinstance(progress, float):\n",
        "          progress = 0\n",
        "      if progress < 0:\n",
        "          progress = 0\n",
        "      if progress >= 1:\n",
        "          progress = 1\n",
        "\n",
        "      block = int(round(bar_length * progress))\n",
        "      \n",
        "\n",
        "      text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
        "      out = widgets.Output()\n",
        "      out.append_stdout(text)\n",
        "\n",
        "\n",
        "  def create_vocab_list(df_in=None):\n",
        "      \"\"\"\n",
        "      Get the glossary\n",
        "      :return:List, each element is a word\n",
        "      \"\"\"\n",
        "      for index, row in df_in.iterrows():\n",
        "          vocab_list.extend(re.findall('\\w+',row['Clean Tweet']))\n",
        "\n",
        "      return list(set(vocab_list))\n",
        "\n",
        "\n",
        "  def get_sim(t, i, j, row_normalized_dt):\n",
        "      '''\n",
        "      Get sim (i, j)\n",
        "      '''\n",
        "      return 1.0 - abs(row_normalized_dt[i,t] - row_normalized_dt[j,t])\n",
        "\n",
        "  def get_Pt(t, tweets_list, friends_tweets_list, row_normalized_dt, relationship):\n",
        "      '''\n",
        "      Get Pt, Pt [i] [j] is the probability that i follows j and i is affected by j under topic t\n",
        "      '''\n",
        "      print('Creating transition probability for topic {}'.format(t))\n",
        "      Pt = scipy.sparse.lil_matrix((relationship.get_shape()))\n",
        "      #rows,cols = relationship.nonzero()\n",
        "      #for row,col in zip(rows,cols):\n",
        "      #    Pt[row,col] = tweets_list[col]/friends_tweets_list[row] * get_sim(t, row, col, row_normalized_dt)\n",
        "      rel_c = relationship.tocoo()    \n",
        "      for i,j in zip(rel_c.row, rel_c.col):\n",
        "          Pt[i,j] = tweets_list[j]/friends_tweets_list[i] * get_sim(t, i, j, row_normalized_dt)\n",
        "      return Pt\n",
        "\n",
        "\n",
        "  def get_TRt(gamma, topic_number, Pt, Et, iter=1000, tolerance=1e-16):\n",
        "      '''\n",
        "      Get TRt, the influence matrix of each user under t topic\n",
        "      :param gamma: Get tuning parameters in TRt's formula\n",
        "      :param Pt: Pt Matrix, Pt [i] [j] represents the probability that i follows j, and i is affected by j under topic t\n",
        "      :param Et: Et Matrix, Et [i] represents user i's attention to topic t, has been normalized, all elements are added to 1\n",
        "      :param iter: Maximum number of iterations\n",
        "      :param tolerance: Stop iteration after TRt iteration when Euclidean distance from iteration is less than tolerance\n",
        "      :return: TRt,TRt[i]Represents the influence of user i under topic t\n",
        "      '''\n",
        "      TRt = Et[:,topic_number]\n",
        "      old_TRt = TRt\n",
        "      i = 0\n",
        "      print('Calculating influence scores for users under topic {}'.format(topic_number))\n",
        "      while i < iter:\n",
        "          TRt = gamma * (Pt*TRt) + (1 - gamma) * Et[:,topic_number]\n",
        "          euclidean_dis = np.linalg.norm(TRt - old_TRt)\n",
        "          if euclidean_dis < tolerance:\n",
        "              break\n",
        "          old_TRt = TRt\n",
        "          i += 1\n",
        "      return TRt\n",
        "\n",
        "\n",
        "  def get_doc_list(df_in):\n",
        "      \"\"\"\n",
        "      Get a list, each element is a piece of document\n",
        "      :param samples: Number of documents\n",
        "      :return: np.array,Each element is a document\n",
        "      \"\"\"\n",
        "      print('Collecting grouped tweets by user')\n",
        "      doc_list = df_in.groupby('screen_name')['Clean Tweet'].apply(lambda x:' '.join(x)).values\n",
        "      return doc_list\n",
        "\n",
        "  def get_feature_matrix(doc_list):\n",
        "      \"\"\"\n",
        "      Get the feature matrix of each document, each word as a feature\n",
        "      :param doc_list: list,Each element is a document\n",
        "      :return: i row and j column list, i is the number of samples, j is the number of features, and feature_matrix_ij represents the number of times that feature j appears in the i-th sample\n",
        "      \"\"\"\n",
        "      print('Creating term-screen_name matrix')\n",
        "      vectorizer = CountVectorizer()\n",
        "      feature_matrix = vectorizer.fit_transform(raw_documents=doc_list)\n",
        "      return feature_matrix, vectorizer\n",
        "\n",
        "\n",
        "  def get_num_tweets_list(nx_graph,df_in):\n",
        "      \"\"\"\n",
        "      Get the number of tweets per user\n",
        "      :return: list,The i element is the number of tweets from the i user\n",
        "      \"\"\"\n",
        "      print('Gathering tweet count for all users')\n",
        "      screen_name = df_in['screen_name'].value_counts()\n",
        "      num_nodes = len(nx_graph)\n",
        "      num_tweets_list = np.zeros(shape=(num_nodes))\n",
        "      for ind,node in enumerate(nx_graph):\n",
        "          num_tweets_list[ind] = screen_name[node] \n",
        "          #update_progress(ind/num_nodes)\n",
        "      return num_tweets_list\n",
        "\n",
        "  def get_relationship(nx_graph):\n",
        "      \"\"\"\n",
        "      Get user relationship matrix\n",
        "      :param samples: Number of Users\n",
        "      :return: i row and j column, relationship [i] [j] = 1 means j follows i\n",
        "      \"\"\"\n",
        "      print('Creating relationship matrix')\n",
        "      return nx.to_scipy_sparse_matrix(nx_graph)\n",
        "\n",
        "\n",
        "  def get_friends_tweets_list(relationship, tweets_list):\n",
        "      \"\"\"\n",
        "      Get the sum of the number of tweets that each user has followed\n",
        "      :param relationship: User relationship matrix, i rows and j columns, relationship [i] [j] = 1 means j follows i\n",
        "      :param tweets_list: list,The i element is the number of tweets from the i user\n",
        "      :return: list,The i element is the sum of the tweets from everyone i followed\n",
        "      \"\"\"\n",
        "      print('Gathering tweet counts for friends of each user')\n",
        "      friends_tweets_list = np.zeros(shape=(relationship.get_shape()[0]))\n",
        "      for i in range(relationship.get_shape()[0]):\n",
        "          friends_tweets_list[i] = tweets_list[relationship[i].nonzero()[1]].sum()\n",
        "          update_progress(i / relationship.get_shape()[0])\n",
        "      return friends_tweets_list\n",
        "\n",
        "  def get_top_topic_influencers(TR, nx_graph, num_topics=5, num_influencers=10):\n",
        "      top_influencer_list = pd.DataFrame()\n",
        "      for i, TRt in enumerate(TR):\n",
        "          top_influencer_list['Topic' + str(i) + 'Influncers'] = pd.Series(TRt,index=nx_graph.nodes()).sort_values(ascending=False).head(num_influencers).index\n",
        "      return top_influencer_list\n",
        "\n",
        "  def get_TR(num_topics, tweets_list, friends_tweets_list, row_normalized_dt, col_normalized_dt, relationship,\n",
        "            gamma=0.2, tolerance=1e-16):\n",
        "      \"\"\"\n",
        "      Get a TR matrix that represents the influence of each user on each topic\n",
        "      :param topics: Number of topics\n",
        "      :param samples: User number\n",
        "      :param tweets_list: list,The i element is the number of tweets from the i user\n",
        "      :param friends_tweets_list: list,The i element is the sum of the tweets from everyone i followed\n",
        "      :param row_normalized_dt: dt Row normalization matrix\n",
        "      :param col_normalized_dt: dt Column normalization matrix\n",
        "      :param relationship: i row and j column, relationship [i] [j] = 1 means j follows i\n",
        "      :param gamma: Get the tuning parameters in the formula for TRt\n",
        "      :param tolerance: Stop iteration after TRt iteration when Euclidean distance from iteration is less than tolerance\n",
        "      :return: list,TR[i][j]Is the influence of user j on topic i\n",
        "      \"\"\"\n",
        "      print('Ranking users by ')\n",
        "      TR = np.zeros(shape=(num_topics,tweets_list.shape[0]))\n",
        "      for i in range(num_topics):\n",
        "          print('topic number {}'.format(i))\n",
        "          Pt = get_Pt(i, tweets_list, friends_tweets_list, row_normalized_dt, relationship)\n",
        "          Et = col_normalized_dt\n",
        "          TR[i] = get_TRt(gamma, i, Pt, Et, tolerance).flatten()\n",
        "      return TR\n",
        "\n",
        "  def get_graph_object(df_in,source='screen_name',target='Retweet of',filter_column=None):\n",
        "      \"\"\"\n",
        "      Get the network in the form of a networkx graph object\n",
        "      :param df_in: The raw dataframe with screen_name and tweets\n",
        "      return: network of screen_name as a networkx object with retweets as edges.\n",
        "      \"\"\"\n",
        "      print('Creating graph object')\n",
        "      G = nx.convert_matrix.from_pandas_edgelist(df_in,source,target)\n",
        "      print('Removing nodes where screen_name doesn\\'t exist in raw df')\n",
        "      G.remove_nodes_from(list(df_in['Retweet of'][~df_in['Retweet of'].isin(df_in['screen_name'].value_counts().index)].unique()))\n",
        "      return G\n",
        "\n",
        "  def get_lda_model(topics, n_iter, df_in):\n",
        "      \"\"\"\n",
        "      Get the trained LDA model\n",
        "      :param topics: Number of topics\n",
        "      :param n_iter: Number of iterations\n",
        "      :return: model,LDA model after training\n",
        "              vocab_list,A list of all words that have appeared in these documents, each element is a word\n",
        "      \"\"\"\n",
        "      doc_list = TwitterRank.get_doc_list(df_in)\n",
        "      #vocab_list = create_vocab_list(df_in)\n",
        "      term_frequency, vectorizer = TwitterRank.get_feature_matrix(doc_list)\n",
        "      #feature_matrix = term_frequency.toarray()\n",
        "      vocab_list = vectorizer.get_feature_names()\n",
        "      print('Fitting LDA model to discover topics')\n",
        "      model = LatentDirichletAllocation(n_components=topics,max_iter=n_iter)\n",
        "      model.fit(term_frequency)\n",
        "      return model, vocab_list, term_frequency\n",
        "\n",
        "\n",
        "  def print_topics_as_df(model, vocab_list, n_top_words=5):\n",
        "\n",
        "      topic_word_df = pd.DataFrame(model.components_,columns=vocab_list)\n",
        "      sorted_topic_words = pd.DataFrame()\n",
        "      for index, row in topic_word_df.iterrows():\n",
        "          row_df = pd.DataFrame({'topic_'+str(index): row.sort_values(ascending=False).index[:5].values})\n",
        "          sorted_topic_words = pd.concat([sorted_topic_words,row_df],axis=1)\n",
        "      return sorted_topic_words\n",
        "\n",
        "\n",
        "  def get_TR_using_DT(dt, df_in, num_topics=5, gamma=0.2, tolerance=1e-16):\n",
        "      \"\"\"\n",
        "      Knowing the DT matrix gives the TR matrix\n",
        "      :param dt: dt The matrix represents the topic distribution of the document, and dt [i] [j] represents the proportion of the topic j in the document i\n",
        "      :param samples: Number of documents\n",
        "      :param topics:  Number of topics\n",
        "      :param gamma: Get the tuning parameters in the formula for TRt\n",
        "      :param tolerance: Stop iteration after TRt iteration when Euclidean distance from iteration is less than tolerance\n",
        "      :return TR: matrix,TR[i][j]Is the influence of user j on topic i\n",
        "      :return nx_graph: A Networkx graph object where the nodes are screen_name and edges are retweets\n",
        "      \"\"\"\n",
        "      row_normalized_dt = dt/(dt.sum(axis=1).reshape(-1,1))\n",
        "      col_normalized_dt = dt/dt.sum(axis=0)\n",
        "      nx_graph = TwitterRank.get_graph_object(df_in,filter_column='Retweet of')\n",
        "      relationship = TwitterRank.get_relationship(nx_graph)\n",
        "      tweets_list = TwitterRank.get_num_tweets_list(nx_graph,df_in)\n",
        "      friends_tweets_list = TwitterRank.get_friends_tweets_list(relationship, tweets_list)\n",
        "      TR = TwitterRank.get_TR(num_topics, tweets_list, friends_tweets_list, row_normalized_dt, col_normalized_dt, relationship,\n",
        "                  gamma, tolerance)\n",
        "      return TR, nx_graph\n",
        "\n",
        "  def twitter_rank(raw_df, topics=5, n_iter=100, gamma=0.2, tolerance=1e-16):\n",
        "      \"\"\"\n",
        "      Twitter rank of documents\n",
        "      :param topics: number of topics to discover\n",
        "      :param n_iter: max number of iterations for LDA model\n",
        "      :param gamma: The tuning parameters in the formula for TRt\n",
        "      :param tolerance: Tolerance for early stopping\n",
        "      :return:TR: A matrix of screen_name influence scores for each topic. Rows are screen_name, columns are topics\n",
        "      :return:graph object: A Networkx graph object where the nodes are screen_name and edges are retweets \n",
        "      \"\"\"\n",
        "      model, vocab_list, term_frequency = get_lda_model(topics, n_iter, raw_df)\n",
        "      #dt matrix represents the topic distribution of the document, \n",
        "      #dt [i] [j] represents the proportion of the subject j in the document i\n",
        "      dt = model._unnormalized_transform(term_frequency)\n",
        "      TR, graph_object = get_TR_using_DT(dt, raw_df, topics, gamma, tolerance)\n",
        "\n",
        "      return TR, graph_object, model, vocab_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "KF9oXDB7XMkC",
        "outputId": "5b8f7724-dc5b-4b3b-b1be-3bc81f7b6525"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-12ddc2f7-930c-484f-837e-daef5b9113d4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-12ddc2f7-930c-484f-837e-daef5b9113d4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving UserTweet_0510.csv to UserTweet_0510.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ-JNvZBXcbd"
      },
      "source": [
        "tweets_df = pd.read_csv('UserTweet_0510.csv')\n",
        "tweets_df = tweets_df.dropna()\n",
        "tweets_df = tweets_df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p57CYN0BYeMO"
      },
      "source": [
        "tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVT-n0b8Yk95"
      },
      "source": [
        "def remove_links(tweet):\n",
        "    tweet = re.sub('http\\S+','',tweet)\n",
        "    tweet = re.sub('(?!http://)bit.ly/\\S+','',tweet)\n",
        "    tweet = re.sub('[A-Za-z0-9.@]+\\.[A-Za-z0-9]+/[A-Za-z0-9.@]+','',tweet)\n",
        "    tweet = tweet.strip('[link]')\n",
        "    return tweet\n",
        "def extract_links(tweet):\n",
        "    link1 = re.findall('http\\S+',tweet)\n",
        "    return list(set(link1.extend(re.findall('(?!http://)bit.ly\\S+',tweet))))\n",
        "def remove_users(tweet):\n",
        "    tweet = re.sub('@[A-Za-z]+[A-Za-z0-9-_]','',tweet)\n",
        "    return tweet\n",
        "def remove_hashtags(tweet):\n",
        "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)','', tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Y2HPy5Ys01"
      },
      "source": [
        "import nltk \n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.Defaults.stop_words.add('rt')\n",
        "nlp.vocab['rt'].is_stop = True\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "\n",
        "# cleaning master function\n",
        "def clean_tweet(tweet, bigrams=False):\n",
        "    tweet = remove_users(tweet)\n",
        "    tweet = remove_links(tweet)\n",
        "    tweet = remove_hashtags(tweet)\n",
        "    tweet = tweet.lower() # lower case\n",
        "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
        "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
        "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
        "    tweet_token_list = [word for word in nlp(tweet)\n",
        "                            if not nlp.vocab[word.text].is_stop] # remove stopwords\n",
        "\n",
        "    tweet_token_list = [word.lemma_ if '#' not in word.text else word.text\n",
        "                        for word in tweet_token_list] # apply lemmatization\n",
        "    tweet = ' '.join(tweet_token_list)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GNs_09SsYxBs"
      },
      "source": [
        "tweets_df['Clean Tweet'] = tweets_df['text'].apply(clean_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwD7NYqYVpTV",
        "outputId": "7af4b815-7edc-4f9e-fa8b-122670bf21e3"
      },
      "source": [
        "def find_retweet_of(tweet):\n",
        "    tweet = re.findall('(?<=http://twitter.com/)\\w+',tweet)\n",
        "    return tweet\n",
        "tweets_df['Retweet of'] = tweets_df['text'].apply(lambda x:find_retweet_of(x) if not pd.isna(x) else x)\n",
        "tweets_df['Retweet of']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        []\n",
              "1        []\n",
              "2        []\n",
              "3        []\n",
              "4        []\n",
              "         ..\n",
              "36270    []\n",
              "36271    []\n",
              "36272    []\n",
              "36273    []\n",
              "36274    []\n",
              "Name: Retweet of, Length: 36275, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8NHfG-yY2lA"
      },
      "source": [
        "def model_topics(algorithm, vocab, model_in=None):\n",
        "    if model_in is None:\n",
        "        if algorithm == 'LDA':\n",
        "            model = LatentDirichletAllocation(n_components=10)\n",
        "        elif algorithm == 'NMF':\n",
        "            model = NMF(n_components=10)\n",
        "        model.fit(tf)\n",
        "        model.get_params()\n",
        "    else: model = model_in\n",
        "    topic_word_df = pd.DataFrame(model.components_,columns=vocab)\n",
        "    sorted_topic_words = pd.DataFrame()\n",
        "    for index, row in topic_word_df.iterrows():\n",
        "        row_df = pd.DataFrame({'topic_'+str(index): row.sort_values(ascending=False).index[:5].values})\n",
        "        sorted_topic_words = pd.concat([sorted_topic_words,row_df],axis=1)\n",
        "    return topic_word_df, sorted_topic_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybU_vMhAPauj"
      },
      "source": [
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def update_progress(progress):\n",
        "    bar_length = 80\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "\n",
        "    block = int(round(bar_length * progress))\n",
        "\n",
        "    clear_output(wait = True)\n",
        "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOiEiR9_PnDe"
      },
      "source": [
        "tweets_df['Clean Tweet'] = tweets_df['Clean Tweet'].apply(lambda x:str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqyOyRrnQFca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3314cf5-b05f-40c2-eed3-c2b5b13e44a1"
      },
      "source": [
        "tweets_df['Clean Tweet']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                   butter\n",
              "1          schedule update   amp happy camp   sbs inkig...\n",
              "2        bts continue melt heart ahead new english lang...\n",
              "3                                   # group teaser photo  \n",
              "4                                        jenna andrews twt\n",
              "                               ...                        \n",
              "36270                                                     \n",
              "36271                                                 vlog\n",
              "36272                                                 vlog\n",
              "36273                                                     \n",
              "36274                                                     \n",
              "Name: Clean Tweet, Length: 36275, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F_EsRcbPyIR",
        "outputId": "750a570b-1202-4acf-b594-970071e8c8f1"
      },
      "source": [
        "model_, vocab_, tf_ = TwitterRank.get_lda_model(5,10,tweets_df)\n",
        "dt = model_._unnormalized_transform(tf_)\n",
        "dt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting grouped tweets by user\n",
            "Creating term-screen_name matrix\n",
            "Fitting LDA model to discover topics\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.02738559e-01, 8.85187750e+02, 2.04024235e-01, 2.00834904e-01,\n",
              "        2.04652335e-01],\n",
              "       [2.02004519e-01, 4.19361178e+00, 2.01134746e-01, 2.01185780e-01,\n",
              "        2.02063175e-01],\n",
              "       [2.00000000e-01, 2.00000000e-01, 2.00000000e-01, 2.00000000e-01,\n",
              "        2.00000000e-01],\n",
              "       ...,\n",
              "       [2.00466453e-01, 2.02769586e-01, 6.18241845e+00, 2.01175239e-01,\n",
              "        6.21317027e+00],\n",
              "       [2.00094839e-01, 2.00039663e-01, 2.00059483e-01, 2.00038067e-01,\n",
              "        1.19976795e+00],\n",
              "       [2.00000000e-01, 2.00000000e-01, 2.00000000e-01, 2.00000000e-01,\n",
              "        2.00000000e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkBcfq2iP1W7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "c7d10aed-2aee-40bc-c7c4-07211a6da95e"
      },
      "source": [
        "TR, graph = TwitterRank.get_TR_using_DT(dt, tweets_df, 5, gamma=0.2, tolerance=1e-16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating graph object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3f728b6dd292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_TR_using_DT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-6584abb2519d>\u001b[0m in \u001b[0;36mget_TR_using_DT\u001b[0;34m(dt, df_in, num_topics, gamma, tolerance)\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0mrow_normalized_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0mcol_normalized_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mnx_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Retweet of'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0mrelationship\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_relationship\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mtweets_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterRank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tweets_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-6584abb2519d>\u001b[0m in \u001b[0;36mget_graph_object\u001b[0;34m(df_in, source, target, filter_column)\u001b[0m\n\u001b[1;32m    185\u001b[0m       \"\"\"\n\u001b[1;32m    186\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating graph object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas_edgelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Removing nodes where screen_name doesn\\'t exist in raw df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Retweet of'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Retweet of'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/networkx/convert_matrix.py\u001b[0m in \u001b[0;36mfrom_pandas_edgelist\u001b[0;34m(df, source, target, edge_attr, create_using, edge_key)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EQBVQAzP41b"
      },
      "source": [
        "final_ranks = TwitterRank.get_top_topic_influencers(TR,graph)\n",
        "final_ranks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iISchkzyP8sr"
      },
      "source": [
        "topics = TwitterRank.print_topics_as_df(model_,vocab_)\n",
        "topics"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}